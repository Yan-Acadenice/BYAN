# Architecture Decision Records (ADR) - BYAN v2.0

**Projet:** BYAN v2.0 - Plateforme d'Orchestration d'Agents IA  
**Auteur:** Yan (avec Winston - Architecte)  
**Date de cr√©ation:** 2026-02-04  
**Version:** 1.0.0  
**Status du document:** Actif  

---

## üéØ Introduction

Ce document regroupe les **Architecture Decision Records (ADR)** pour BYAN v2.0. Chaque ADR documente une d√©cision architecturale critique prise lors de la phase de conception, avec son contexte, sa justification, ses cons√©quences et les alternatives √©valu√©es.

### Pourquoi les ADR ?

Les ADR permettent de:
- **Tracer** l'historique des d√©cisions techniques
- **Justifier** les choix architecturaux aupr√®s des contributeurs
- **√âviter** de remettre en question des d√©cisions d√©j√† prises
- **Faciliter** l'onboarding de nouveaux d√©veloppeurs
- **Documenter** les compromis (trade-offs) accept√©s

### Format des ADR

Chaque ADR suit le format standard:
- **Date:** Date de la d√©cision
- **Status:** Accepted | Proposed | Deprecated | Superseded
- **Context:** Contexte et probl√®me √† r√©soudre
- **Decision:** D√©cision prise
- **Rationale:** Arguments et justifications d√©taill√©s
- **Consequences:** Impacts positifs et n√©gatifs
- **Alternatives Considered:** Autres options √©valu√©es

---

## ADR-001: Node.js au lieu de Python

**Date:** 2026-02-04  
**Status:** Accepted  

### Context

BYAN v1.x existant est d√©velopp√© en Node.js et distribu√© via NPX (`npx create-byan-agent`). La plateforme v2.0 n√©cessite de g√©rer:
- Orchestration asynchrone complexe (agents, workers, workflows)
- Distribution via NPM/NPX pour simplicit√© d'installation
- Compatibilit√© avec la base de code existante
- Performance sur des op√©rations I/O intensives

Python est souvent consid√©r√© comme le langage de pr√©dilection pour l'IA/ML, mais cela introduirait une rupture technologique.

### Decision

**Conserver Node.js (>= 18.0.0) avec JavaScript pur comme stack technique principal.**

### Rationale

**Arguments en faveur de Node.js:**

1. **Coh√©rence avec l'existant:**
   - BYAN v1.x est en Node.js
   - Pas de r√©√©criture compl√®te n√©cessaire
   - R√©utilisation de modules existants (_bmad/core/, _bmad/bmm/)
   - Exp√©rience utilisateur pr√©serv√©e (NPX)

2. **Async/Await natif:**
   - Event loop Node.js parfait pour orchestration
   - Gestion native des promesses et op√©rations concurrentes
   - Worker pool et dispatcher = use-case id√©al pour event-driven

3. **√âcosyst√®me NPM:**
   - Packages matures: `js-yaml`, `winston`, `node-cache`, `commander`
   - Distribution triviale: `npm install -g` ou `npx`
   - 0 friction d'installation pour utilisateurs

4. **Performance I/O:**
   - Lecture/√©criture YAML workflows: non-blocking I/O
   - Appels API LLM parall√®les: optimis√© pour latence r√©seau

5. **Developer Experience:**
   - JavaScript = barri√®re entr√©e basse
   - Pas de setup Python virtual env
   - Single runtime (Node.js)

**Compromis accept√©s:**

- Pas de biblioth√®ques ML natives (scikit-learn, pandas)
- Moins d'outils de data science mature
- Typage optionnel (pas de TypeScript pour MVP)

### Consequences

**Positives:**
- ‚úÖ Continuit√© technologique (pas de Big Bang rewrite)
- ‚úÖ Time-to-market r√©duit (7 jours au lieu de 3-4 semaines)
- ‚úÖ R√©utilisation code existant (~60% de _bmad/core/)
- ‚úÖ Distribution NPM/NPX simple
- ‚úÖ Event loop optimal pour orchestration asynchrone

**N√©gatives:**
- ‚ùå Pas de ML natif (Phase 2: appel API Python ou TensorFlow.js)
- ‚ùå Pas de typage strict (acceptable pour MVP)

**Impact sur la roadmap:**
- Phase 2 (ML-based dispatcher): Utilisation de TensorFlow.js ou micro-service Python
- Phase 3 (Agent adaptatif): Idem, API Python via child process

### Alternatives Considered

**Option A: Python**
- **Avantages:** √âcosyst√®me ML/IA (scikit-learn, transformers), typage (mypy), data science
- **Inconv√©nients:** Rupture tech stack, r√©√©criture compl√®te, setup virtual env complexe, distribution PyPI moins fluide que NPX
- **Verdict:** ‚ùå Rejet√© - co√ªt de migration trop √©lev√© pour MVP

**Option B: Architecture Hybride (Node.js + Python)**
- **Avantages:** Best of both worlds, Node pour orchestration, Python pour ML
- **Inconv√©nients:** Complexit√© op√©rationnelle, 2 runtimes √† installer, communication inter-process overhead
- **Verdict:** ‚ùå Rejet√© pour MVP - acceptable Phase 2+ si ML n√©cessaire

**Option C: TypeScript**
- **Avantages:** Typage strict, meilleure maintenabilit√©
- **Inconv√©nients:** Compilation transpilation step, complexit√© setup, overhead mental pour MVP
- **Verdict:** üü° Envisag√© Phase 2 - JavaScript pur suffisant pour MVP

---

## ADR-002: In-Memory Cache au lieu de Redis

**Date:** 2026-02-04  
**Status:** Accepted  

### Context

Le Context Layer doit g√©rer:
- Chargement fr√©quent des fichiers YAML de contexte (platform.yaml, project.yaml, story.yaml)
- R√©solution de placeholders r√©p√©titive ({user_name}, {output_folder})
- Op√©rations de lecture I/O co√ªteuses (disk access)

Objectif: **R√©duire latence de chargement de context < 50ms** (crit√®re success HYPER-MVP).

Options de cache:
- **In-memory** (node-cache, lru-cache)
- **Redis** (cache distribu√© persistant)
- **Aucun cache** (lecture fichier √† chaque fois)

### Decision

**Utiliser `node-cache` (in-memory cache) comme solution de cache L1.**

### Rationale

**Arguments en faveur de node-cache:**

1. **0 d√©pendance externe:**
   - Pas de Redis server √† installer/configurer
   - Pas de docker-compose pour d√©veloppeurs
   - Installation: `npm install node-cache` ‚Üí 1 commande

2. **Performance:**
   - Access time: **< 1ms** (RAM locale)
   - vs Redis: 1-5ms (network roundtrip localhost)
   - Context loading: 50ms ‚Üí 10ms avec cache (80% am√©lioration)

3. **Simplicit√© d'impl√©mentation:**
   ```javascript
   const NodeCache = require('node-cache');
   const cache = new NodeCache({ stdTTL: 600 }); // 10min TTL
   
   async loadContext(level, id) {
     const cacheKey = `context:${level}:${id}`;
     let context = cache.get(cacheKey);
     if (!context) {
       context = await this._loadFromDisk(level, id);
       cache.set(cacheKey, context);
     }
     return context;
   }
   ```

4. **Resource efficiency:**
   - Memory footprint: **< 50MB** pour cache complet
   - vs Redis: 100MB+ (serveur standalone)
   - CPU: 0% idle (cache RAM = simple object lookup)

5. **MVP appropri√©:**
   - Single process Node.js
   - Pas de scaling horizontal pour MVP
   - Cache hit rate 60%+ suffisant

**Compromis accept√©s:**

- **Perte de donn√©es au restart** (cache volatile)
  - Acceptable: context YAML = source of truth sur disque
  - Warm-up au d√©marrage: < 2s pour charger tous les contextes
  
- **Pas de partage multi-process**
  - Acceptable pour MVP (single CLI process)
  - Phase 2: Redis si mode server HTTP d√©ploy√©

### Consequences

**Positives:**
- ‚úÖ Setup instantan√© (0 config externe)
- ‚úÖ Latence cache < 1ms (vs 1-5ms Redis)
- ‚úÖ Memory usage: < 50MB
- ‚úÖ Simplicit√© code (10 lignes impl√©mentation)
- ‚úÖ 0 point de d√©faillance externe

**N√©gatives:**
- ‚ùå Cache perdu au restart (non-probl√©matique car source = disk)
- ‚ùå Pas de persistance (acceptable car contexte = fichiers YAML)
- ‚ùå Single-process only (Phase 2: multi-process = Redis)

**Impact sur architecture:**
- Context Layer reste l√©ger et autonome
- Pas de d√©pendance infrastructure pour d√©veloppeurs
- √âvolution naturelle vers Redis Layer 2 en Phase 2 si scaling n√©cessaire

### Alternatives Considered

**Option A: Redis**
- **Avantages:** Persistance, partage multi-process, distribution, TTL avanc√©
- **Inconv√©nients:** Setup complexe (docker/install), overhead network, 100MB+ RAM, overkill pour MVP
- **Verdict:** ‚ùå Rejet√© pour MVP - envisag√© Phase 2 si mode server HTTP

**Option B: Aucun cache**
- **Avantages:** Code simple, 0 d√©pendance, toujours √† jour
- **Inconv√©nients:** Latence inacceptable (50ms+ par load), I/O disque r√©p√©t√©, CPU overhead YAML parsing
- **Verdict:** ‚ùå Rejet√© - performance insuffisante (crit√®re < 50ms non respect√©)

**Option C: LRU-Cache**
- **Avantages:** Algorithme √©viction Least Recently Used, lightweight
- **Inconv√©nients:** Pas de TTL automatique, gestion manuelle taille
- **Verdict:** üü° √âquivalent node-cache - node-cache choisi pour API plus riche (TTL, stats)

---

## ADR-003: Rule-Based Dispatcher au lieu de ML

**Date:** 2026-02-04  
**Status:** Accepted  

### Context

Le **Economic Dispatcher** doit router les t√¢ches entre:
- **Workers** (Haiku-like, rapides, √©conomiques) pour t√¢ches simples
- **Agent** (Sonnet-like, puissants, co√ªteux) pour t√¢ches complexes

Objectif: **40-50% r√©duction des requ√™tes co√ªteuses** vers l'agent.

Crit√®res de complexit√© √† √©valuer:
- Longueur du prompt (tokens)
- Type de t√¢che (validation vs architecture)
- Taille du contexte
- Keywords de complexit√© (analyze, design, optimize)

Options:
- **ML-based routing** (mod√®le entra√Æn√©)
- **Rule-based scoring** (algorithme d√©terministe)
- **Random routing** (baseline)

### Decision

**Impl√©menter un algorithme rule-based avec scoring d√©terministe pour le dispatcher MVP.**

### Rationale

**Arguments en faveur du rule-based:**

1. **Pas de dataset d'entra√Ænement:**
   - BYAN v1 n'a pas collect√© de donn√©es de routing
   - Cr√©er un dataset = 2-3 semaines de data labeling
   - Pas de ground truth (quelle t√¢che = worker vs agent)

2. **Simplicit√© et pr√©dictibilit√©:**
   ```javascript
   calculateComplexity(task) {
     let score = 0;
     // Facteur 1: Tokens (max 30 points)
     score += Math.min(tokenCount / 100, 30);
     // Facteur 2: Type t√¢che (0-80 points)
     score += TASK_COMPLEXITY[task.type];
     // Facteur 3: Context size (max 20 points)
     score += Math.min(contextSize / 5000, 20);
     // Facteur 4: Keywords (5 points each)
     score += complexKeywords.length * 5;
     return Math.min(score, 100);
   }
   ```

3. **Debuggable et explicable:**
   - Score visible dans logs: `complexity_score: 45`
   - Facile √† ajuster (tuning des poids)
   - Pas de "black box" ML

4. **Performance acceptable:**
   - **Baseline estim√©e: 70% accuracy** sur t√¢ches simples vs complexes
   - Am√©lioration it√©rative facile (ajout de r√®gles)
   - Worker fallback si √©chec

5. **Foundation pour ML Phase 2:**
   - Logging de tous les routings (task ‚Üí executor ‚Üí success)
   - Collecte de dataset automatique (6-12 mois)
   - Entra√Ænement mod√®le supervis√© avec labels r√©els

**Compromis accept√©s:**

- **Accuracy < 100%** (70% attendu, vs 90%+ avec ML)
  - Mitigation: Worker fallback vers Agent si √©chec
  - Acceptable pour MVP: √©conomie 40-50% d√©j√† atteinte

- **Pas d'adaptation automatique**
  - R√®gles = statiques (pas de self-learning)
  - Phase 2: ML remplacera rule-based

### Consequences

**Positives:**
- ‚úÖ Impl√©mentation rapide (1 jour au lieu de 2-3 semaines ML)
- ‚úÖ 0 d√©pendance ML framework (TensorFlow, scikit-learn)
- ‚úÖ Debuggable et explicable (logs transparents)
- ‚úÖ Accuracy 70%+ suffisante pour MVP
- ‚úÖ Collecte donn√©es pour ML Phase 2

**N√©gatives:**
- ‚ùå Accuracy limit√©e √† 70-80% (vs 90%+ ML potentiel)
- ‚ùå Pas d'auto-am√©lioration (tuning manuel)
- ‚ùå R√®gles statiques (pas d'adaptation usage patterns)

**Impact sur roadmap:**
- Phase 1 (MVP): Rule-based dispatcher op√©rationnel J3-4
- Phase 2 (Mois 2-3): ML model entra√Æn√© sur donn√©es collect√©es
- Phase 3 (Mois 4-6): Dispatcher adaptatif (self-optimizing)

### Alternatives Considered

**Option A: ML-based routing (mod√®le supervis√©)**
- **Avantages:** Accuracy 90%+, adaptation automatique, patterns complexes
- **Inconv√©nients:** Pas de dataset (3 semaines de prep), overhead runtime (TensorFlow.js 50MB), black box debugging
- **Verdict:** ‚ùå Rejet√© pour MVP - envisag√© Phase 2 avec donn√©es r√©elles

**Option B: Random routing (baseline)**
- **Avantages:** Ultra simple, 0 logique
- **Inconv√©nients:** 50% accuracy random (inacceptable), pas d'√©conomie
- **Verdict:** ‚ùå Rejet√© - baseline utile uniquement pour benchmark

**Option C: Heuristique simple (if tokens > 500 ‚Üí agent)**
- **Avantages:** Encore plus simple que scoring
- **Inconv√©nients:** Trop simpliste (type de t√¢che ignor√©), accuracy 60% estim√©e
- **Verdict:** üü° Insuffisant - scoring multi-facteurs n√©cessaire

---

## ADR-004: Worker Pool Statique au lieu de Dynamique

**Date:** 2026-02-04  
**Status:** Accepted  

### Context

Le **Worker Pool** g√®re l'ex√©cution de t√¢ches simples via workers (mod√®les l√©gers type Haiku).

Questions:
- **Combien de workers instancier ?** (1, 2, N dynamique)
- **Auto-scaling ?** (cr√©er/d√©truire workers selon charge)
- **Strat√©gie d'allocation ?** (round-robin, least-busy, queue)

Contraintes MVP:
- CLI single-user (1 utilisateur √† la fois)
- Workflows s√©quentiels (rarement > 2 t√¢ches parall√®les)
- Resource-constrained (laptop developer)

### Decision

**Impl√©menter un Worker Pool statique de 2 workers fixes, sans auto-scaling.**

### Rationale

**Arguments en faveur du pool statique:**

1. **Simplicit√© d'impl√©mentation:**
   ```javascript
   class WorkerPool {
     constructor(size = 2) {
       this.workers = Array.from({ length: size }, (_, i) => 
         new Worker(i)
       );
     }
     
     async getAvailableWorker() {
       let worker = this.workers.find(w => w.isAvailable());
       if (!worker) {
         await this.waitForWorker(); // simple polling
         worker = this.workers.find(w => w.isAvailable());
       }
       return worker;
     }
   }
   ```

2. **Ressources pr√©visibles:**
   - 2 workers = **~100MB RAM** (vs N workers = impr√©visible)
   - CPU: 2 threads max concurrents
   - Pas de spike de cr√©ation/destruction

3. **Adapt√© au use-case MVP:**
   - CLI = 1 workflow √† la fois
   - Workflows BYAN = rarement > 2 steps parall√®les
   - Exemple workflow "Create PRD":
     ```yaml
     steps:
       - id: extract_info (worker 1)
       - id: format_template (worker 2) # parall√®le possible
       - id: final_review (agent) # s√©quentiel
     ```

4. **Performance suffisante:**
   - Queue wait time: < 2s (crit√®re success)
   - Throughput: 2 tasks/s (largement suffisant CLI)
   - 0 overhead de scheduling complexe

5. **√âconomie de complexit√©:**
   - Pas de logique auto-scaling (health checks, thresholds, cooldown)
   - Pas de worker lifecycle management
   - Code: 50 lignes au lieu de 300+

**Compromis accept√©s:**

- **Pas de scaling automatique**
  - Si 3+ t√¢ches parall√®les ‚Üí queue (wait < 2s acceptable)
  - Mitigation: Workflows bien con√ßus = max 2 steps parall√®les
  
- **Ressources "gaspill√©es" si idle**
  - 2 workers = 100MB m√™me si inactifs
  - Acceptable: worker = lightweight (pas de mod√®le charg√© en m√©moire)

### Consequences

**Positives:**
- ‚úÖ Impl√©mentation simple (50 lignes de code)
- ‚úÖ RAM pr√©visible: 100MB pour pool
- ‚úÖ Performance suffisante: < 2s wait time
- ‚úÖ 0 overhead de scaling logic
- ‚úÖ Debugging trivial (2 workers = 2 √©tats √† tracker)

**N√©gatives:**
- ‚ùå Pas de scaling si charge √©lev√©e (acceptable CLI single-user)
- ‚ùå 100MB RAM "gaspill√©e" si idle (compromis acceptable)
- ‚ùå Hard-coded 2 workers (configurable via ENV Phase 2)

**Impact sur architecture:**
- Worker Pool reste simple et robuste
- Phase 2: Config ENV `BYAN_WORKER_POOL_SIZE=4` si besoin
- Phase 3: Auto-scaling si mode server HTTP d√©ploy√©

### Alternatives Considered

**Option A: Auto-scaling dynamique**
- **Avantages:** Resource efficiency, handle spikes, optimal utilisation
- **Inconv√©nients:** Complexit√© 10x (health checks, thresholds, cooldown), overhead cr√©ation/destruction, debugging difficile
- **Verdict:** ‚ùå Rejet√© pour MVP - overkill pour CLI single-user

**Option B: Single worker (N=1)**
- **Avantages:** Ultra simple, 50MB RAM
- **Inconv√©nients:** 0 parall√©lisme (workflows 2x plus lents), queue wait > 5s inacceptable
- **Verdict:** ‚ùå Rejet√© - performance insuffisante

**Option C: N=4 workers**
- **Avantages:** Plus de parall√©lisme
- **Inconv√©nients:** 200MB RAM (overhead), rarement utilis√©s √† pleine capacit√©, diminishing returns
- **Verdict:** üü° Over-provisioning - N=2 optimal pour CLI use-case

**Option D: Worker pool configurable (ENV variable)**
- **Avantages:** Flexibilit√© utilisateur
- **Inconv√©nients:** Complexit√© config, users = mauvais tuning
- **Verdict:** üü° Envisag√© Phase 2 - N=2 hardcoded suffisant MVP

---

## ADR-005: Workflow YAML au lieu de Code

**Date:** 2026-02-04  
**Status:** Accepted  

### Context

Les **Workflows** orchestrent des s√©quences multi-√©tapes pour accomplir des t√¢ches m√©tier (ex: "Create PRD", "Generate Architecture").

Questions:
- **Format de d√©finition ?** (YAML, JSON, JavaScript code)
- **D√©claratif vs imp√©ratif ?**
- **Flexibilit√© vs simplicit√© ?**

Exigences:
- Developer Experience: non-codeurs doivent pouvoir cr√©er workflows
- √âvolutivit√©: ajout de steps sans red√©ployer
- Lisibilit√©: claire pour humains et outils
- Expressivit√©: conditions, retry, fallback

### Decision

**Adopter un DSL (Domain-Specific Language) d√©claratif au format YAML pour d√©finir les workflows.**

### Rationale

**Arguments en faveur de YAML DSL:**

1. **Developer Experience sup√©rieure:**
   ```yaml
   # Workflow lisible par humains
   name: create-simple-prd
   version: 1.0.0
   
   steps:
     - id: extract_requirements
       type: worker
       input: "Extract key requirements from: {user_input}"
       output_file: "{output_folder}/requirements.md"
       retry:
         max_attempts: 2
         
     - id: generate_prd
       type: agent
       agent: architect
       input: "Create PRD based on: {step.extract_requirements.output}"
       output_file: "{output_folder}/PRD.md"
   ```

2. **Pas de red√©ploiement:**
   - Workflows = fichiers s√©par√©s dans `_bmad/workflows/`
   - Modification workflow ‚Üí pas de `npm install` ou rebuild
   - Hot-reload possible (Phase 2)

3. **Validation et tooling:**
   - Schema YAML validable (JSON Schema)
   - IDE support (YAML IntelliSense)
   - Diff/merge workflows (Git friendly)

4. **Expressivit√© suffisante:**
   - Variables: `{user_input}`, `{step.previous.output}`
   - Conditions: Phase 2 (if/else YAML)
   - Retry policy: `max_attempts`, `backoff`
   - Parallel steps: `depends_on: []`

5. **S√©paration concerns:**
   - Logique m√©tier (workflow YAML) ‚â† Ex√©cution (WorkflowExecutor.js)
   - Business users = edit YAML
   - Developers = code executor engine

**Compromis accept√©s:**

- **Parser YAML n√©cessaire** (dependency: `js-yaml`)
  - Overhead: < 10ms parsing
  - Acceptable: 1 parsing au d√©but du workflow

- **Validation runtime** (pas compile-time)
  - Erreurs d√©tect√©es √† l'ex√©cution
  - Mitigation: Schema validator pre-run (Phase 2)

### Consequences

**Positives:**
- ‚úÖ DX excellente (lisibilit√©, √©dition facile)
- ‚úÖ Pas de red√©ploiement pour nouveaux workflows
- ‚úÖ Git-friendly (diff/merge YAML)
- ‚úÖ S√©paration logique m√©tier / code
- ‚úÖ Extensible (ajout keywords: conditions, loops Phase 2)

**N√©gatives:**
- ‚ùå Parser YAML n√©cessaire (dependency js-yaml ~50KB)
- ‚ùå Validation runtime (erreurs √† l'ex√©cution)
- ‚ùå Moins expressif que code JavaScript (acceptable trade-off)

**Impact sur architecture:**
- WorkflowExecutor = core engine (r√©utilisable)
- Workflows = assets d√©coupl√©s (facile √† versionner)
- Contributeurs non-dev peuvent cr√©er workflows

### Alternatives Considered

**Option A: Code JavaScript (imp√©ratif)**
```javascript
// workflow-create-prd.js
module.exports = async (context) => {
  const requirements = await worker.execute({
    input: `Extract from: ${context.user_input}`
  });
  const prd = await agent.execute({
    input: `Create PRD: ${requirements}`
  });
  return prd;
};
```
- **Avantages:** Full expressivit√© JavaScript, typage possible, debugger natif
- **Inconv√©nients:** Red√©ploiement n√©cessaire, barri√®re entr√©e codeurs only, moins lisible
- **Verdict:** ‚ùå Rejet√© - DX inf√©rieure, rigidit√©

**Option B: JSON d√©claratif**
```json
{
  "name": "create-simple-prd",
  "steps": [
    {"id": "extract", "type": "worker", "input": "..."}
  ]
}
```
- **Avantages:** Parsing natif (`JSON.parse`), validation stricte
- **Inconv√©nients:** Verbeux (quotes partout), pas de commentaires, moins lisible
- **Verdict:** üü° Acceptable mais YAML > JSON pour DX

**Option C: DSL custom (syntaxe propri√©taire)**
```
WORKFLOW create-prd
  STEP extract USING worker
    INPUT "Extract from {user_input}"
  END
END
```
- **Avantages:** Syntaxe ultra concise
- **Inconv√©nients:** Parser custom complexe, 0 IDE support, pas standard
- **Verdict:** ‚ùå Rejet√© - over-engineering, YAML = standard industrie

**Option D: Hybrid (YAML config + JavaScript callbacks)**
- **Avantages:** YAML pour structure, JS pour logique complexe
- **Inconv√©nients:** Complexit√© mentale (2 langages), moins d√©claratif
- **Verdict:** üü° Envisag√© Phase 3 si besoins logique complexe

---

## üìä Synth√®se des D√©cisions

| ADR | D√©cision | Impact Principal | Phase |
|-----|----------|------------------|-------|
| **001** | Node.js vs Python | Continuit√© tech stack, 60% code r√©utilis√© | MVP ‚úÖ |
| **002** | In-Memory vs Redis | Setup 0 config, latence < 1ms | MVP ‚úÖ |
| **003** | Rule-Based vs ML | Impl√©mentation 1 jour, accuracy 70%+ | MVP ‚úÖ |
| **004** | Static vs Dynamic Pool | 100MB RAM, simplicit√© 50 lignes | MVP ‚úÖ |
| **005** | YAML vs Code | DX sup√©rieure, pas de red√©ploiement | MVP ‚úÖ |

### Trade-offs Globaux Accept√©s

**Simplicit√© > Sophistication** (pour MVP):
- In-memory cache au lieu de Redis
- Rule-based au lieu de ML
- Static pool au lieu de auto-scaling

**Developer Experience > Performance th√©orique**:
- YAML (lisible) au lieu de JSON (parsable)
- Node.js (familier) au lieu de Python (ML natif)

**Time-to-Market > Perfection**:
- 7 jours MVP au lieu de 3-4 semaines "optimal"
- 70% accuracy dispatcher acceptable (vs 90% ML)

---

## üöÄ √âvolution Future des ADR

### Phase 2 (Semaines 2-4)
- **ADR-006:** Redis Layer 2 Cache (multi-process)
- **ADR-007:** ML Dispatcher avec dataset collect√©
- **ADR-008:** Worker Auto-Scaling (server mode)

### Phase 3 (Mois 2-3)
- **ADR-009:** TypeScript Migration
- **ADR-010:** Workflow Conditions & Loops
- **ADR-011:** Agent Plugins Architecture

### Phase 4 (Mois 4-6)
- **ADR-012:** Self-Optimizing Routing
- **ADR-013:** Distributed Tracing (OpenTelemetry)
- **ADR-014:** Agent Adaptive Learning

---

## üìö R√©f√©rences

**Documents li√©s:**
- `byan-v2-0-architecture-node.md` - Architecture technique d√©taill√©e
- `byan-v2-requirements.md` - Requirements fonctionnels
- `05-analyse-impacts.md` - Analyse d'impacts m√©tier

**Standards ADR:**
- [ADR GitHub Template](https://adr.github.io/)
- [Documenting Architecture Decisions (Michael Nygard)](https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions)

---

**Document cr√©√© le:** 2026-02-04  
**Derni√®re mise √† jour:** 2026-02-04  
**Auteur:** Yan (avec Winston - Architecte)  
**Status:** Actif  
**Version:** 1.0.0
